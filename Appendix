# Appendix A — Prompt Templates & Reproducibility README

> **Purpose:** This appendix collects the exact prompt templates used during the ontology construction process and a reproducibility README with step-by-step instructions to run the pipeline, regenerate the ontology, validate it, and reproduce the evaluation experiments.

---

## Table of contents

* Appendix A.1 — Prompt usage guidelines
* Appendix A.2 — Prompt templates (copy-paste ready)

  * A.2.1 Domain specification
  * A.2.2 Competency question generation
  * A.2.3 Class & property extraction
  * A.2.4 OWL / RDF/XML generation
  * A.2.5 Error handling & refinement prompts
* Appendix A.3 — Naming / IRI policy
* Appendix B — Reproducibility README

  * B.1 Environment & dependencies
  * B.2 File layout (repo)
  * B.3 How to run the pipeline (commands)
  * B.4 Validation: OOPS! & HermiT (how-to)
  * B.5 Expected outputs & troubleshooting

---

## Appendix A.1 — Prompt usage guidelines

* **Role priming:** Start prompts by assigning the model a role (e.g., `Act as a legal ontology engineer with experience in OWL and RDFLib`). This reduces hallucinations and encourages structured output.
* **Structure and format:** Always request machine-parsable, minimal-ambiguity outputs. Use explicit format instructions (JSON, YAML, or Turtle-like blocks) and show a short example output (few-shot) when possible.
* **Chunking:** For long legal texts, split the input into manageable chunks (by chapter or group of articles). Keep a manifest that maps chunk → prompt call so you can re-run a single chunk later.
* **Context passing:** Include only necessary context and a short manifest (list of already-defined IRIs) if you want the model to preserve identity across multiple calls.
* **Post-processing:** Expect to run scripts that canonicalize IRIs, remove duplicates, and validate RDF/XML. Don’t rely on the model for final cleaning.

---

## Appendix A.2 — Prompt templates (copy-paste ready)

> **Note:** Replace `{{PLACEHOLDER}}` tokens with actual values. Keep the prompt size reasonable; for long texts use chunking.

### A.2.1 Domain specification (single-shot)

```
System: You are a legal ontology engineer. You will read a short excerpt of an EU regulation and identify its structural elements (preamble/recitals, chapters, sections, articles, annexes) and propose candidate domain concepts.

User: Context: Regulation reference: "Regulation (EU) 2024/1689 — AI Act". Excerpt: {{CHUNK_TEXT}}

Task: Produce a JSON with three fields: {
  "toc_entries": [ {"type": "Chapter|Section|Article|Annex|Recital", "id": "article_10", "title": "Article 10 - Data and data governance", "summary": "..." } , ... ],
  "candidate_concepts": ["HighRiskAI","AI_System","Provider","User", ...],
  "suggested_relations": [ {"label":"imposes","domain":"Authority","range":"Penalty"}, ... ]
}

Provide only the JSON block and nothing else.
```

### A.2.2 Competency questions (few-shot)

```
System: You are a legal ontology engineer who writes competency questions (CQs) for ontology-driven systems. For each CQ include an identifier and a short justification.

User: Based on the following concepts: {{CONCEPT_LIST}} generate 8-12 competency questions relevant for compliance checking and governance. Output format: JSON list of {"id":"CQ1","question":"...","justification":"..."} only.
```

### A.2.3 Class & property extraction (structured)

```
System: You are an ontology generator. Given the following article text, identify candidate classes, properties (object/datatype), and named individuals. For each property give a suggested domain and range and whether it should be an object or datatype property.

User: Article text: {{ARTICLE_TEXT}}

Output format (JSON):
{
 "classes":[{"iri":"{{candidateIRI}}","label":"...","comment":"...","superclass":"..."}],
 "object_properties":[{"iri":"{{propIRI}}","label":"...","domain":"...","range":"...","comment":"..."}],
 "data_properties":[{"iri":"{{dpIRI}}","label":"...","domain":"...","rangeDatatype":"xsd:string"}],
 "individuals":[{"iri":"{{indIRI}}","label":"...","type":"...","comment":"..."}]
}

Instruction: Provide only valid JSON. Use concise IRIs (no spaces). Prefer camelCase for labels.
```

### A.2.4 OWL / RDF/XML generation (Turtle optional)

```
System: You are a code generator for RDF/OWL. Convert the following JSON ontology fragment into Turtle (preferred) or RDF/XML. Maintain the provided IRIs and annotations. Use standard prefixes: rdf, rdfs, owl, xsd, schema, dpv, and a project namespace https://w3id.org/example/aiact#

User: JSON fragment: {{JSON_FRAGMENT}}

Task: Output a Turtle document containing the triples to define classes, subclass relations, objectProperties (with domain/range), and individuals. Do not include extraneous commentary.
```

### A.2.5 Error handling & refinement prompts

```
System: You are an assistant for ontology debugging. The following Turtle file failed the RDFLib parser with this error: {{PARSER_ERROR}}. The file content is: {{TURTLE_TEXT}}

Task: Suggest up to 5 minimal edits to fix syntax issues and produce a corrected minimal Turtle fragment. Output only the corrected Turtle.
```

---

## Appendix A.3 — Naming / IRI policy (recommended)

* **Namespace:** `https://w3id.org/<your-project>/<ontology>#` (e.g., `https://w3id.org/charikleia/aiact#`).
* **Class IRIs:** Use CamelCase, e.g., `HighRiskAI`, `AI_System` → prefer `AISystem` or `HighRiskAI`.
* **Article / Recital individuals:** Use `article_10` or `recital_42` (snake\_case) but map them to readable `rdfs:label` values: `rdfs:label "Article 10 - Data and data governance"@en`.
* **Object / Data properties:** Use verb-based camelCase, e.g., `imposesPenalty`, `hasConformityAssessment`.
* **Avoid:** spaces, double underscores, punctuation in IRIs; canonicalize digits (Article 10 -> article\_10).

---

# Appendix B — Reproducibility README

## B.1 Environment & dependencies

Minimum tested environment (example):

* Python 3.10+ (3.12 recommended)
* pip

Required Python packages (example `requirements.txt`):

```text
openai==0.27.0  # or the version you use
rdflib==6.4.0
lxml
owlready2
pandas
pytest (optional for tests)
python-dotenv
```

Install:

```bash
python -m venv venv
source venv/bin/activate   # Linux / macOS
venv\Scripts\activate     # Windows
pip install -r requirements.txt
```

If you used a different LLM client (e.g., `openai` vs `openai-client`), adjust accordingly.

## B.2 File layout (suggested repo)

```
/aiact-ontology-repo/
├── README.md
├── prompts/
│   ├── domain_spec_prompt.txt
├── python_code/
│   ├── ontology_classification.py     
│   ├── hierchy.py           # chunk -> JSON extractor
├── ontology4.0.owl
├── appendix/
```

## B.3 How to run the pipeline (commands)

1. **Set credentials:** create a `.env` file with your API keys (if using OpenAI):

```text
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxx
```

2. **Run the full pipeline (example):**

```bash
python python_code/ontology_classification.py --input ontology.txt --output ontology.txt
```

4. **Combine TTLs and serialize to OWL/XML:**

```bash
python scripts/merge_graphs.py --inputs outputs/*.ttl --out outputs/ontology4.0.owl
```

5. **Load in Protégé:** open `outputs/ontology4.0.owl` with Protégé (File → Open) for visualization and manual edits.

## B.4 Validation: OOPS! & HermiT (how-to)

### OOPS! (OntOlogy Pitfall Scanner)

1. Go to the OOPS! online service: [https://oops.linkeddata.es/](https://oops.linkeddata.es/) (or use a local installation if available).
2. Upload `outputs/ontology4.0.owl` or paste the Turtle.
3. Inspect the list of pitfalls and severity, and save the report (JSON or HTML).
4. Create a `appendix/oops_report.md` that lists the top 10 pitfalls and the corrective actions you applied (with diffs).

### HermiT (Protégé reasoner)

1. Open `ontology4.0.owl` in Protégé.
2. In Protégé: Reasoner → HermiT (select) → Start reasoner.
3. Check for inconsistent classes, unsatisfiable classes, and unexpected inferences.
4. Save the Protégé snapshot and include a screenshot or the `*.omni` file in `appendix/`.

Include both OOPS! and HermiT outputs in the appendix to show how you corrected issues discovered.


## B.5 Expected outputs & troubleshooting

* **Expected files after a successful run:** `outputs/ontology4.0.owl`
* **Common errors & fixes:**

  * *RDFLib parse errors:* run canonicalize\_iris.py to remove illegal characters, then reserialize.
  * *Duplicated IRIs:* run dedupe step (scripts/canonicalize\_iris.py) to merge or rename duplicates.
  * *Model forgetting IRIs across prompts:* include a persistent manifest (`data/ai_act_toc.json`) and pass `existing_iris` in the prompt to preserve identity.

